{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71106ca",
   "metadata": {},
   "source": [
    "### This notebook is used to train a  deep learning model for prediction tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb69a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AACB', 'AACBR', 'AACBU', 'AACG', 'AADR', 'AAL', 'AAME', 'AAOI', 'AAON', 'AAPB']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from yahoo_fin import stock_info as si\n",
    "import torch.nn as nn\n",
    "\n",
    "#test - get some tickers(AKA stock symbol)\n",
    "nasdaq_tickers = si.tickers_nasdaq()\n",
    "print(nasdaq_tickers[:10])  \n",
    "\n",
    "sp500_tickers = si.tickers_sp500()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202dd0e",
   "metadata": {},
   "source": [
    "### Ideas: \n",
    "1. First we will find some popular stock (Apple, Tesla, ....) an easy way is to select based on total market value\n",
    "2. Through yahoo fin api we will get their historical data\n",
    "3. For each stock, we will get `k` lastest timestamp (day, minutes, ...) OHLCV, which create `k` x `5` 2D matrix (`k` here is sequence_length below, which present how far in the past we look back to predict the future values)\n",
    "4. We will get about 30-60 samples for each stock (as much as possible), for example if `k` = 15, we will get the value from day 1-15 as first sample, 16-30 as the second one, and so on .....\n",
    "5. All of them will be feeded into the model, we will get the final models after training process.\n",
    "\n",
    "Estimation: 50 stocks, each stock will have ~ 100 samples, `k` = 15 -> 50 x 100 x 15 x 5 \n",
    "\n",
    "Train/valid/test ratio = 7/1.5/1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b786a7",
   "metadata": {},
   "source": [
    "# Model decleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPriceModel(nn.Module):\n",
    "    def __init__(self, sequence_length=30, \n",
    "                 input_features=5, \n",
    "                 hidden_size=128, \n",
    "                 num_layers=2, \n",
    "                 dropout=0.2):\n",
    "        super(StockPriceModel, self).__init__()\n",
    "        \n",
    "        # Input: [batch_size, sequence_length, input_features]\n",
    "        # Where sequence_length = k, input_features = 5 (OHLC) + 1 (VOLUME)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer now predicts all 5 features (OHLCV)\n",
    "        self.fc2 = nn.Linear(64, out_features= input_features) #output size = input size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        lstm_out = lstm_out.permute(1, 0, 2)  # [k, batch_size, hidden_size]\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Return to original shape\n",
    "        attn_out = attn_out.permute(1, 0, 2)  # [batch_size, k, hidden_size]\n",
    "        \n",
    "        # Use the last time step for prediction\n",
    "        out = self.fc1(attn_out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Output all 5 features\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440c734",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c38fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 10\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # MSE across all 5 output features (OHLCV)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss/len(val_loader):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1bd809",
   "metadata": {},
   "source": [
    "# Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_mse = 0.0\n",
    "    feature_mse = [0.0, 0.0, 0.0, 0.0, 0.0]  # MSE for each of Open, High, Low, Close and Volume\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Overall MSE\n",
    "            mse = F.mse_loss(outputs, targets)\n",
    "            total_mse += mse.item()\n",
    "            \n",
    "            # Individual feature MSE\n",
    "            for i in range(5):\n",
    "                feature_mse[i] += F.mse_loss(outputs[:, i], targets[:, i]).item()\n",
    "    \n",
    "    # Calculate average\n",
    "    avg_mse = total_mse / len(test_loader)\n",
    "    avg_feature_mse = [mse / len(test_loader) for mse in feature_mse]\n",
    "    \n",
    "    print(f\"Overall MSE: {avg_mse:.6f}\")\n",
    "    print(f\"Open MSE: {avg_feature_mse[0]:.6f}\")\n",
    "    print(f\"High MSE: {avg_feature_mse[1]:.6f}\")\n",
    "    print(f\"Low MSE: {avg_feature_mse[2]:.6f}\")\n",
    "    print(f\"Close MSE: {avg_feature_mse[3]:.6f}\")\n",
    "    print(f\"Volume MSE: {avg_feature_mse[4]:.6f}\")\n",
    "    \n",
    "    return avg_mse, avg_feature_mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
